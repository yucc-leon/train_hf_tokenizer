import os, json
from datasets import load_dataset
from tqdm import tqdm
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)


os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"
# os.environ['HF_HOME'] = "/home/aigc/.cache/huggingface"

corpus_for_tokenizer = load_dataset("./datasets/to_train_tokenizer_sample")


print("corpus info:")
print(corpus_for_tokenizer)


def get_training_corpus():
    for i in range(0, 10000, 1000):
        yield corpus_for_tokenizer['train'][i : i+1000]['text']


special_tokens = [
    "<ï½œbeginâ–ofâ–textï½œ>",
    "<ï½œendâ–ofâ–textï½œ>",
    "<ï½œUserï½œ>", 
    "<ï½œAssistantï½œ>", 
    "<ï½œfimâ–middleï½œ>",
    "<ï½œfimâ–prefixï½œ>",
    "<ï½œfimâ–suffixï½œ>",
    "<ï½œapiâ–callsâ–beginï½œ>",
    "<ï½œapiâ–callsâ–endï½œ>",
    "<ï½œapiâ–callâ–beginï½œ>",
    "<ï½œapiâ–callâ–endï½œ>",
    "<ï½œapiâ–outputsâ–beginï½œ>",
    "<ï½œapiâ–outputsâ–endï½œ>",
    "<ï½œapiâ–outputâ–beginï½œ>",
    "<ï½œapiâ–outputâ–endï½œ>",
    "<ï½œapiâ–sepï½œ>",
]        



tokenizer = Tokenizer(models.BPE())


tokenizer.normalizer = normalizers.Sequence([])

# preprocessing copied from deepseekV2
tokenizer.pre_tokenizer = pre_tokenizers.Sequence(
    [
        pre_tokenizers.Split('[\r\n]', "isolated"),   
        pre_tokenizers.Split("\s?[A-Za-zÂµÃ€-Ã–Ã˜-Ã¶Ã¸-ÆºÆ¼-Æ¿Ç„-Ê“Ê•-Ê¯Í°-Í³Í¶Í·Í»-Í½Í¿Î†Îˆ-ÎŠÎŒÎ-Î¡Î£-ÏµÏ·-ÒÒŠ-Ô¯Ô±-Õ–á‚ -áƒ…á -áµá¸-á½á²-á²ºá²½-á²¿á´€-á´«áµ«-áµ·áµ¹-á¶šá¸€-á¼•á¼˜-á¼á¼ -á½…á½ˆ-á½á½-á½—á½™á½›á½á½Ÿ-á½½á¾€-á¾´á¾¶-á¾¼á¾¾á¿‚-á¿„á¿†-á¿Œá¿-á¿“á¿–-á¿›á¿ -á¿¬á¿²-á¿´á¿¶-á¿¼â„‚â„‡â„Š-â„“â„•â„™-â„â„¤â„¦â„¨â„ª-â„­â„¯-â„´â„¹â„¼-â„¿â……-â…‰â…â†ƒâ†„â°€-â±»â±¾-â³¤â³«-â³®â³²â³³ê™€-ê™­êš€-êš›êœ¢-ê¯ê±-ê‡ê‹-êê­°-ê®¿ï¬€-ï¬†ï¬“-ï¬—ï¼¡-ï¼ºï½-ï½šğ€-ğ‘ğ’°-ğ““ğ“˜-ğ“»ğ²€-ğ²²ğ³€-ğ³²ğ‘¢ -ğ‘£Ÿğ¤€-ğ¥ƒ]+","isolated"), 
        pre_tokenizers.Split('\s?[!-/:-~ï¼-ï¼ï¼š-ï½â€˜-â€Ÿã€€-ã€‚]+', "isolated"),   
        pre_tokenizers.Split('\s+$', "isolated"), 
        pre_tokenizers.Split("[ä¸€-é¾¥à €-ä¸€ê°€-íŸ¿]+","isolated"),
        pre_tokenizers.Digits(individual_digits=True),
        pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False),
    ])
tokenizer.post_processor = processors.ByteLevel(use_regex=True, trim_offsets=False, add_prefix_space=True)
tokenizer.decoder = decoders.ByteLevel(use_regex=True, trim_offsets=True, add_prefix_space=True)



print("start training...")
trainer = trainers.BpeTrainer(vocab_size=1024, special_tokens=special_tokens)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)


from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<ï½œbeginâ–ofâ–textï½œ>",
    eos_token="<ï½œendâ–ofâ–textï½œ>",
    pad_token="<ï½œendâ–ofâ–textï½œ>"
)

wrapped_tokenizer.save_pretrained("./64k_tokenizer")